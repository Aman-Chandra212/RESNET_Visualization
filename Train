BATCH_SIZE = 8

def train(model, train_loader, EPOCHS = 20, lossF = None ):
  optim = torch.optim.Adam(model.parameters(), lr = 5e-4, weight_decay=1e-4)
  if lossF == None:
    lossF = nn.CrossEntropyLoss()
  model.train()

  for epoch in range(EPOCHS):
    correct = 0
    for batch_idx, (X, y) in enumerate(train_loader):
      
      #Move data to device
      var_X = Variable(X).to(device) 
      var_y = Variable(y).to(device)
      # print(var_X_batch.size())
      
      ## Forward Pass!
      output = model(var_X)

      ## Calculate the loss incurred
      loss = lossF(output, var_y.long())
      
      ## BackProp: Computes all gradients.
      loss.backward()
      
      ## Gradient Descent Step (Adam)
      optim.step()
      optim.zero_grad() # This is important because PyTorch keeps on adding to the original value of gradient.

      ## Gets the predictions. From probablities (the digit with highest probablity is the prediction) 
      predicted = torch.max(output.data, axis = 1).indices
      
      correct += (predicted == var_y).sum()

      checkpoint = {'model': model,
          'state_dict': model.state_dict(),
          'optimizer' : optim.state_dict()}

      torch.save(checkpoint, default_path + 'checkpoint.pth')

      if (batch_idx % 200) == 0:
          print('Epoch : {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\t Accuracy:{:.3f}%'.format(
                    epoch, batch_idx*len(X), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))



def test(model, test_loader):
    """
    Change the value of correct to contain the number of correctly classified test examples out of the 10000 example in test_loader.
    """
    correct = 0 
    # Put your code here: 
    # __begin
    for batch_idx , (X_batch , y_batch) in tqdm(enumerate(test_loader)):
       var_X_batch = Variable(X_batch).to(device) 
       var_y_batch = Variable(y_batch).to(device)

       output = model(var_X_batch)
       predicted = torch.max(output.data, axis=1).indices
       if predicted == var_y_batch:
         correct += 1

  
    #__end
    print("Test accuracy:{:.3f}% ".format( float(correct * 100) / (len(test_loader))))

def train_im(model, t_data,  o_class = 0 ,iters = 1000, lossF=nn.CrossEntropyLoss()):

  im = torch.zeros_like(t_data[1][0]).view(1, 3, 224, 224).to(device)
  im = Variable(im, requires_grad = True)
  o_class = Variable(torch.tensor(o_class)).to(device).view(1)
  optim = torch.optim.Adam ([im,], lr = 5e-4, weight_decay=1e-4)
  for i in tqdm(range(iters)): 
    output = model(im)
    loss = lossF(output, o_class)
    loss.backward()
    optim.step()
    optim.zero_grad


    
  return im
